import streamlit as st
import asyncio
import json
import re
from twikit import Client
from datetime import datetime

# --- Ø¯ Ù¾Ø§Ú¼Û ØªÙ†Ø¸ÛŒÙ…Ø§Øª ---
st.set_page_config(
    page_title="Ø¯ Ø§Ù„ÛŒØ§Ø³ Ø³Ú©Ø±ÛŒÙ¾Ø± - Ø¢Ù†Ù„Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡",
    page_icon="ğŸ¦",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Ù…Ø±Ø³ØªÙ†Ø¯ÙˆÛŒÙ‡ ÙÙ†Ú©Ø´Ù†ÙˆÙ†Ù‡ ---
def clean_tweet_content(text):
    text = re.sub(r'https?://\S+', '', text)
    text = re.sub(r'www\.\S+', '', text)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'[ \t]+', ' ', text)
    return text.strip()

def extract_hashtags(text):
    return re.findall(r'#\w+', text)

async def scrape_process(queries, limit, ct0, auth_token, post_type, sort_mode):
    client = Client('en-US')
    # Ú©ÙˆÚ©ÛŒØ² ØªÙ†Ø¸ÛŒÙ…ÙˆÙ„
    client.set_cookies({"ct0": ct0, "auth_token": auth_token})
    
    all_results = []
    seen_content_hashes = set()
    global_count = 0
    status_text = st.empty()
    progress_bar = st.progress(0)

    try:
        for q_idx, query in enumerate(queries):
            if global_count >= limit: break
            
            status_text.text(f"ğŸ” Ù„Ù¼ÙˆÙ† Ø±ÙˆØ§Ù† Ø¯ÛŒ: {query}...")
            
            try:
                tweets = await client.search_tweet(query, product=post_type, count=limit)
            except Exception as e:
                st.error(f"Error searching {query}: {e}")
                continue

            if not tweets:
                continue

            while tweets:
                for tweet in tweets:
                    if global_count >= limit: break
                    
                    original_text = tweet.text
                    clean_text = clean_tweet_content(original_text)
                    
                    if not clean_text or len(clean_text) < 5: continue

                    text_hash = hash(clean_text)
                    if text_hash in seen_content_hashes: continue
                    seen_content_hashes.add(text_hash)
                    
                    tags = extract_hashtags(original_text)
                    global_count += 1
                    
                    post_obj = {
                        "PostNo": str(global_count),
                        "MyPost": clean_text,
                        "Tags": ", ".join(tags)
                    }
                    all_results.append(post_obj)
                    
                    # Ù¾Ø±Ù…Ø®ØªÚ« ÚšÙˆØ¯Ù„
                    progress = min(global_count / limit, 1.0)
                    progress_bar.progress(progress)

                if global_count >= limit: break
                
                if hasattr(tweets, 'next'):
                    try: tweets = await tweets.next()
                    except: break
                else: break
        
        # ØªØ±ØªÛŒØ¨ (Sorting)
        if sort_mode == "Shortest First":
            all_results.sort(key=lambda x: len(x["MyPost"]))
        elif sort_mode == "Longest First":
            all_results.sort(key=lambda x: len(x["MyPost"]), reverse=True)
            
        # Ø´Ù…ÛØ±Û Ø³Ù…ÙˆÙ„
        for idx, item in enumerate(all_results):
            item["PostNo"] = str(idx + 1)
            
        status_text.text("âœ… Ù¾Ø±ÙˆØ³Ù‡ Ø¨Ø´Ù¾Ú“Ù‡ Ø´ÙˆÙ‡!")
        progress_bar.progress(100)
        return all_results

    except Exception as e:
        st.error(f"Ø³ØªØ±Ù‡ ØªÛØ±ÙˆØªÙ†Ù‡: {e}")
        return []

# --- Ø¯ ÙˆÛŒØ¨Ù¾Ø§Ú¼Û Ú‰ÛŒØ²Ø§ÛŒÙ† (GUI) ---
st.title("ğŸš€ Ø¯ Ø§Ù„ÛŒØ§Ø³ Ø¯ Ø³Ú©Ø±ÛŒÙ¾ Ú©ÙˆÙ„Ùˆ Ø¢Ù†Ù„Ø§ÛŒÙ† Ø³ÛŒØ³Ù¼Ù…")

# Ø³Ø§ÛŒÚ‰Ø¨Ø§Ø± (Ú©ÛŒÚ¼ Ø§Ú“Ø® ØªÙ‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª)
with st.sidebar:
    st.header("ğŸ”‘ Ø¯ Ú©ÙˆÚ©ÛŒØ² Ù…Ø¯ÛŒØ±ÛŒØª")
    ct0_val = st.text_input("CT0 Ú©ÙˆÚ‰:", value="", type="password")
    auth_val = st.text_input("Auth Token:", value="", type="password")
    
    st.markdown("---")
    st.header("âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª")
    search_type = st.selectbox("Ø¯ Ù¾Ù„Ù¼Ù†Û Ú‰ÙˆÙ„", ["Latest", "Top"])
    sort_algo = st.selectbox("ØªØ±ØªÛŒØ¨ (Sort)", ["None", "Shortest First", "Longest First"])
    limit_count = st.number_input("Ø¯ Ù¾ÙˆØ³Ù¼ÙˆÙ†Ùˆ ØªØ¹Ø¯Ø§Ø¯", min_value=10, max_value=500, value=50)

# Ø§ØµÙ„ÙŠ Ø¨Ø±Ø®Ù‡
st.subheader("ğŸ” Ø¯Ù„ØªÙ‡ Ø®Ù¾Ù„ Ù‡Ø´Ù¼Ø§Ú«ÙˆÙ†Ù‡ ÙˆÙ„ÛŒÚ©Ø¦")
query_text = st.text_area("Ù‡Ø± Ù‡Ø´Ù¼Ø§Ú« Ù¾Ù‡ Ù†ÙˆÛ Ú©Ø±ÚšÙ‡ Ú©Û ÙˆÙ„ÛŒÚ©Ø¦:", "#Ø®Ù„Ø§ÙØª_ÛŒÙˆØ§Ø²ÛŒÙ†ÛŒ_Ø§Ù†ØªØ®Ø§Ø¨\n#Ø§ÙØºØ§Ù†Ø³ØªØ§Ù†")

col1, col2 = st.columns([1, 2])

with col1:
    start_btn = st.button("Ù¾ÛŒÙ„ Ú©Ú“Ø¦ (Start Scraping)", type="primary")

# Ú©Ù„Ù‡ Ú†Û Ø¨Ù¼Ù† ÙˆÙˆÙ‡Ù„ Ø´ÙŠ
if start_btn:
    if not ct0_val or not auth_val:
        st.warning("Ù…Ù‡Ø±Ø¨Ø§Ù†ÙŠ ÙˆÚ©Ú“Ø¦ Ù„ÙˆÙ…Ú“ÛŒ CT0 Ø§Ùˆ Auth Token Ø¯Ù†Ù†Ù‡ Ú©Ú“Ø¦!")
    else:
        queries = [q.strip() for q in query_text.split('\n') if q.strip()]
        
        # Ø¯ Async ÙÙ†Ú©Ø´Ù† Ú†Ù„ÙˆÙ„
        results = asyncio.run(scrape_process(queries, limit_count, ct0_val, auth_val, search_type, sort_algo))
        
        if results:
            st.success(f"Ù…Ø¨Ø§Ø±Ú©! {len(results)} Ù¾ÙˆØ³Ù¼ÙˆÙ†Ù‡ Ù¾ÛŒØ¯Ø§ Ø´ÙˆÙ„.")
            
            # Ú‰ÛŒÙ¼Ø§ ÚšÙˆØ¯Ù„
            st.dataframe(results)
            
            # Ø¯ Ú‰Ø§ÙˆÙ†Ù„ÙˆÚ‰ Ø¨Ù¼Ù† Ø¬ÙˆÚ“ÙˆÙ„
            json_str = json.dumps(results, ensure_ascii=False, indent=4)
            st.download_button(
                label="ğŸ“¥ ÙØ§ÛŒÙ„ Ú‰Ø§ÙˆÙ†Ù„ÙˆÚ‰ Ú©Ú“Ø¦ (JSON)",
                data=json_str,
                file_name="scraped_data.json",
                mime="application/json"
            )
        else:
            st.warning("Ù‡ÛŒÚ… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆÙ†Ù‡ Ù…ÙˆÙ†Ø¯Ù„ Ø´ÙˆÙ„ ÛŒØ§ ØªÛØ±ÙˆØªÙ†Ù‡ Ø±Ø§Ù…Ù†ÚØªÙ‡ Ø´ÙˆÙ‡.")
